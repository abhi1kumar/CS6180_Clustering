{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "501bfc2618fa33abeaa94cfc2104d3cd",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem Set 3: HAC\n",
    "\n",
    "This problem set departs a bit from the usual in that we're going to use `scipy`'s hierarchical clustering tools. `Scikit-learn` has `AgglomerativeClustering`, but the `scipy.cluster.hierarchy` tools give us dendrograms, which are nice for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2fadbe731a46061e7a3d6a018c4f2c54",
     "grade": false,
     "grade_id": "boilerplate",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "# Some scaling functions\n",
    "from sklearn.preprocessing import robust_scale, minmax_scale, maxabs_scale, scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable high resolution PNGs\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': '#DFDFE5'}\n",
    "sns.set(context='notebook', style='darkgrid', rc=rc)\n",
    "\n",
    "# We're going to use scipy for visualizations and clustering\n",
    "from scipy.cluster import hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c881a383c9358e630349d3af27c31ef",
     "grade": false,
     "grade_id": "data-load-clean-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "I've included an [HTML file](Data-Cleaning-Tutorial.html) that describes in detail how I loaded and cleaned the dataset. If you're interested, you should look this over to see what I did. It might also be helpful for your project.\n",
    "\n",
    "To sum up, \n",
    "1. I load the dataset that I [downloaded from here](http://catalog.data.gov/dataset/ssa-disability-claim-data), using `pd.read_csv`, making sure to specify the thousands separator as a comma (see [the tutorial](Data-Cleaning-Tutorial.html) for why).\n",
    "2. I drop any column with one unique value, because it will tell us nothing, and any column that has an \"object\" type.\n",
    "3. I drop any row with `na` values.\n",
    "\n",
    "Then I use `head` to look at the first few columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2798490650cd15bc02553824d61960b8",
     "grade": false,
     "grade_id": "data-load-clean",
     "locked": true,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('SSA-SA-FYWL.csv', thousands=',')\n",
    "df_clean = df.drop([col for col, data in df.iteritems() if data.dtype == np.object or data.nunique() == 1], axis=1).dropna()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99ff67b7b0a57b8201ac7f8852d8153c",
     "grade": false,
     "grade_id": "p1-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "We're plotting some dendrograms in this assignment. Remember that a *dendrogram* is a visualization of the HAC process that tells us how the clusters are formed. It's a tree, and the leaves are the individual examples. The tree branches where two clusters are joined. The *height* where the branch happens is the distance between the two clusters, which depends on the metric we use and the method of joining clusters. \n",
    "\n",
    "Let's make a function that gives us a nice big dendrogram so we can see what's going on, and plots it. Then let's plot a dendrogram for each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8bd0ff5eb278e9a73fe2cd6703b539b8",
     "grade": false,
     "grade_id": "p1-code",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_big_dendro(data, title=None, scale_fn=None, method='ward', n_clusters=1):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.title(title)\n",
    "    if scale_fn is None:\n",
    "        Z = hierarchy.linkage(data, method=method)\n",
    "    else:\n",
    "        Z = hierarchy.linkage(scale_fn(data), method=method)\n",
    "    dn = hierarchy.dendrogram(Z, color_threshold=Z[1-n_clusters,2], distance_sort=True, no_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "plot_big_dendro(df_clean.values, title=\"Single linkage\", method='single')\n",
    "plot_big_dendro(df_clean.values, title=\"Complete linkage\", method='complete')\n",
    "plot_big_dendro(df_clean.values, title=\"Average linkage\", method='average')\n",
    "plot_big_dendro(df_clean.values, title=\"Ward linkage\", method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c54388a0be6c7401f79a6485a1b07df9",
     "grade": false,
     "grade_id": "p1-desc-2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Given what you know about linkages, compare the dendrograms above. Remember that height corresponds to distance between clusters. What does it mean for a cluster to be \"high and narrow\"? Or \"low and wide\"? Is complete linkage the best *for this data*? Why or why not? Is single the best *for this data*? Why or why not? Would you use single linkage for some applications and complete for others? What about average linkage? Or Ward linkage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f5993fa45528f40863a3e56587fe917",
     "grade": true,
     "grade_id": "p1-ans",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9b8b60e4eee51b355478e5f74ae1fba2",
     "grade": false,
     "grade_id": "p2-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "In addition to what we already see above, we can also preprocess the data to scale all of the features before we try to cluster the data. We might want to do this because certain features in the original dataset are naturally larger than others -- for example, any \"population\" feature will be naturally larger than a \"percentage\" feature. \n",
    "\n",
    "There are several ways to do this, of which these are only a few:\n",
    "1. `scale`, which normalizes each feature by converting to its Z score (subtract mean and divide by standard deviation)\n",
    "2. `robust_scale`, which does something similar, but uses median and interquartile range\n",
    "3. `minmax_scale`, which contracts the values linearly to the range [0,1], so that the min is 0 and the max is 1\n",
    "4. `maxabs_scale`, which simply divides by the maximum absolute value of the feature. This is good for preserving sparsity, because it doesn't affect zero values.\n",
    "\n",
    "Let's compare them, and also to one without scaling, and use Ward linkage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e7dd4df157fc8858aee76f3b379cfe7",
     "grade": false,
     "grade_id": "p2-code",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_big_dendro(df_clean.values, title=\"Just a dendrogram, Ma'am\")\n",
    "plot_big_dendro(df_clean.values, title=\"Scale max absolute value\", scale_fn=maxabs_scale)\n",
    "plot_big_dendro(df_clean.values, title=\"Scale min and max to range\", scale_fn=minmax_scale)\n",
    "plot_big_dendro(df_clean.values, title=\"Scale using Z score\", scale_fn=scale)\n",
    "plot_big_dendro(df_clean.values, title=\"Scale using robust scaling\", scale_fn=robust_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c60115bd3167e191562dd9a8389dd687",
     "grade": false,
     "grade_id": "p2-desc-2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Compare the above dendrograms. In the spirit of Problem 1, what would you say about them? Is any one scaling method preferable? Would you use *any* of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7678895f9629500177f25db5d5bdcfb2",
     "grade": true,
     "grade_id": "p2-ans",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7d69df87e1af73a99bf0832f39b26439",
     "grade": false,
     "grade_id": "p3-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "We can also color the dendrogram with different colors for different clusters. The function above just chooses the right cluster join from the linkage information and colors everything below that distance. Every connected component below the threshold gets a different color.\n",
    "\n",
    "Run the code below a few different times and play with the number of clusters, the linkage method, and the scaling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = 8\n",
    "scale_fn = None # One of None, scale, robust_scale, minmax_scale, maxabs_scale\n",
    "method = 'ward' # One of 'single', 'complete', 'average', and 'ward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "cdaab2db203606ac8d4c5dc4f2362e2a",
     "grade": false,
     "grade_id": "p3-code",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_big_dendro(df_clean.values, title=\"Dendrogram with colors\", n_clusters=n_clusters, scale_fn=scale_fn, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "258ec336d85d36a9c60c5e95247998f6",
     "grade": false,
     "grade_id": "p3-desc-2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "What are the settings that you feel give the best clustering *for this data*? What makes these choices better than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "32e8a175ac3772ac65e8d74edec86666",
     "grade": true,
     "grade_id": "p3-ans",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdd1457e33f8312946ea6ae064ffa2be",
     "grade": false,
     "grade_id": "p4-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "Now we're going to extract a labeling given the same three parameters. Then we'll join that information with the *original* data, that still has state and region codes. We'll start with the same parameters that you selected before, but you can change this later. We also include a stacked bar plot, broken out by region. The height is the count in each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = n_clusters\n",
    "scale_fn = scale_fn # One of None, scale, robust_scale, minmax_scale, maxabs_scale\n",
    "method = method # One of 'single', 'complete', 'average', and 'ward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "53d970210d615ea353299faff705cfda",
     "grade": false,
     "grade_id": "p4-code",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_hac_labels(data, scale_fn=None, method='ward', n_clusters=8):\n",
    "    if scale_fn is None:\n",
    "        Z = hierarchy.linkage(data, method=method)\n",
    "    else:\n",
    "        Z = hierarchy.linkage(scale_fn(data), method=method)\n",
    "    return hierarchy.cut_tree(Z, n_clusters=n_clusters).squeeze()\n",
    "\n",
    "labels = get_hac_labels(df_clean.values, scale_fn=scale_fn, method=method, n_clusters=n_clusters)\n",
    "label_df = pd.DataFrame(labels, index=df_clean.index, columns=['label'])\n",
    "label_df = df.join(label_df)\n",
    "\n",
    "column = 'Region Code'\n",
    "sns.set_palette('Set2', n_clusters)\n",
    "label_df.loc[:,[column, 'label']].pivot_table(\n",
    "    index=column, columns='label', aggfunc=len\n",
    ").plot.bar(figsize=(15, 5), stacked=True, width=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d13df9ba145db3242d9bd55f46e465a",
     "grade": false,
     "grade_id": "p4-desc2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Play with the parameters above. Do any parameters make any more sense than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00ab2f7d3aa7dceb4d247d7e15a8e1d8",
     "grade": true,
     "grade_id": "p4-ans",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e62bdcd7a9449157366eca79db82cf2b",
     "grade": false,
     "grade_id": "p5-desc",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 5\n",
    "\n",
    "Now we'll do the same thing, except that we'll break it out by state. Again, play with the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = n_clusters\n",
    "scale_fn = scale_fn # One of None, scale, robust_scale, minmax_scale, maxabs_scale\n",
    "method = method # One of 'single', 'complete', 'average', and 'ward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9f5a2696ab076c7aeb9f498b72585ee6",
     "grade": false,
     "grade_id": "p5-code",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "column = 'State Code'\n",
    "sns.set_palette('Set2', n_clusters)\n",
    "label_df.loc[:,[column, 'label']].pivot_table(\n",
    "    index=column, columns='label', aggfunc=len\n",
    ").plot.bar(figsize=(15, 5), stacked=True, width=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "86da99837bae0df275c393aa9e49dc06",
     "grade": false,
     "grade_id": "p5-desc-2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Something surprising happens with the states. All of them should be nearly the same height, 15, except for PR, which is 13 (there were `nan` values in those data points). But there is nearly no variation within most of the states. Why is this? Also, when you play with the parameters, can you say anything about the clusters that occur? Why some states land in some colors with other states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc2274ccb4803e66a82274950cbf2918",
     "grade": true,
     "grade_id": "p5-ans",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
